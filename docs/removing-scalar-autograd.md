# Removing scalar-autograd: Migration to Analytical Gradients

## Overview

This document outlines the steps to remove the `scalar-autograd` dependency and replace automatic differentiation with analytical gradients generated by gradient-script. This migration will improve performance (no tape recording overhead) while maintaining the proven solver infrastructure.

**Key insight**: We never materialize the dense Jacobian. Instead, each provider accumulates directly into J^T J and J^T r, exploiting the natural sparsity of the problem.

## Current Architecture

### What scalar-autograd Does

1. **Value class**: Wraps numbers and tracks computation graph
2. **Backward pass**: Computes gradients via reverse-mode autodiff
3. **Jacobian computation**: In `autodiff-dense-lm.ts`, `computeJacobian()` calls `.backward()` on each residual to get gradients w.r.t. all variables

### Current Flow (in `transparentLM`)

```
Variables (Value[]) → Residual Function → Residuals (Value[])
                                              ↓
                              For each residual: .backward()
                                              ↓
                              Extract gradients from variables
                                              ↓
                              Build DENSE Jacobian matrix (M×N)  ← bottleneck
                                              ↓
                              Convert to sparse, compute J^T J
                                              ↓
                              Solve normal equations
```

### Target Flow (analytical)

```
Providers[] → For each provider:
                 - Compute residual (plain number)
                 - Compute gradient (array of ~10 values)
                 - Accumulate into J^T J (touches ~10×10 block)
                 - Accumulate into J^T r (touches ~10 entries)
                                              ↓
              Solve normal equations (same as before)
```

**Key difference**: Never build the M×N Jacobian. A reprojection residual depends on ~10 variables, so it contributes a 10×10 block to J^T J directly.

### What We Keep

- `transparentLM()` solver infrastructure (LM loop, damping, convergence)
- Sparse matrix operations (`SparseMatrix`, `conjugateGradientDamped`)
- `ConstraintSystem` orchestration
- Entity-driven architecture (points/lines/constraints add themselves)

## Analytical Gradients Available

The following gradient functions are already generated in `src/optimization/residuals/gradients/`:

| Residual Type | Gradient Files |
|---------------|----------------|
| Reprojection U | `reprojection-u-gradient.ts`, `reprojection-u-dcam-gradient.ts` |
| Reprojection V | `reprojection-v-gradient.ts`, `reprojection-v-dcam-gradient.ts` |
| Distance | `distance-gradient.ts` |
| Angle | `angle-gradient.ts` |
| Collinear | `collinear-gradient.ts`, `collinear-x/y/z-gradient.ts` |
| Coplanar | `coplanar-gradient.ts` |
| Fixed Point | `fixed-point-gradient.ts`, `fixed-point-x/y/z-gradient.ts` |
| Line Direction | `line-direction-*-gradient.ts` (9 variants) |
| Line Length | `line-length-gradient.ts` |
| Coincident Point | `coincident-point-x/y/z-gradient.ts` |
| Quaternion Norm | `quat-norm-gradient.ts` |
| Vanishing Line | `vanishing-line-gradient.ts` |

---

## Migration Steps (Incremental with Verification)

Each phase is small, testable, and maintains backward compatibility until the final cleanup.

---

### Phase 1: Interfaces Only (No Runtime Change)

**Goal**: Define types. Zero behavior change.

Create `src/optimization/analytical/types.ts`:

```typescript
/**
 * A provider computes one residual and its gradient.
 * Knows which variables it depends on (sparse pattern).
 */
export interface AnalyticalResidualProvider {
  /** Variable indices this residual depends on (e.g., [0,1,2,7,8,9,10,11,12,13] for reprojection) */
  readonly variableIndices: readonly number[];

  /** Compute residual value (plain number, not Value) */
  computeResidual(variables: Float64Array): number;

  /** Compute gradient w.r.t. each variable in variableIndices (same length as variableIndices) */
  computeGradient(variables: Float64Array): Float64Array;
}

/**
 * Maps entities to variable indices.
 * Built once at solve() start, immutable during optimization.
 */
export interface VariableLayout {
  /** Total number of free variables */
  readonly numVariables: number;

  /** Initial variable values */
  readonly initialValues: Float64Array;

  /** Returns indices for a world point's [x, y, z]. -1 if locked. */
  getWorldPointIndices(pointId: string): readonly [number, number, number];

  /** Returns indices for camera position [x, y, z]. -1 if locked. */
  getCameraPosIndices(cameraId: string): readonly [number, number, number];

  /** Returns indices for quaternion [w, x, y, z]. */
  getCameraQuatIndices(cameraId: string): readonly [number, number, number, number];
}
```

**Verification**:
- `npm run build` passes
- No test changes needed
- No runtime behavior change

---

### Phase 2: Direct J^T J Accumulation Function

**Goal**: Implement the core sparse accumulation. Test in isolation.

Create `src/optimization/analytical/accumulate-normal-equations.ts`:

```typescript
import { SparseMatrix, Triplet } from '../sparse/SparseMatrix';
import { AnalyticalResidualProvider } from './types';

export interface NormalEquations {
  /** J^T J as sparse matrix */
  JtJ: SparseMatrix;
  /** -J^T r (negative gradient of cost) */
  negJtr: Float64Array;
  /** Sum of squared residuals */
  cost: number;
  /** Individual residual values (for debugging) */
  residuals: Float64Array;
}

/**
 * Accumulates J^T J and J^T r directly from providers.
 * Never materializes the full Jacobian.
 */
export function accumulateNormalEquations(
  variables: Float64Array,
  providers: readonly AnalyticalResidualProvider[]
): NormalEquations {
  const n = variables.length;
  const m = providers.length;

  const triplets: Triplet[] = [];
  const negJtr = new Float64Array(n);  // Will be negated at end
  const residuals = new Float64Array(m);
  let cost = 0;

  for (let p = 0; p < m; p++) {
    const provider = providers[p];
    const r = provider.computeResidual(variables);
    const grad = provider.computeGradient(variables);
    const idx = provider.variableIndices;

    residuals[p] = r;
    cost += r * r;

    // Accumulate into J^T J (symmetric, so store both (i,j) and (j,i))
    for (let i = 0; i < idx.length; i++) {
      const vi = idx[i];
      if (vi < 0) continue;  // Locked variable

      for (let j = i; j < idx.length; j++) {
        const vj = idx[j];
        if (vj < 0) continue;  // Locked variable

        const contrib = grad[i] * grad[j];
        triplets.push({ row: vi, col: vj, value: contrib });
        if (vi !== vj) {
          triplets.push({ row: vj, col: vi, value: contrib });
        }
      }

      // Accumulate into J^T r
      negJtr[vi] -= grad[i] * r;  // Negative because we want descent direction
    }
  }

  const JtJ = SparseMatrix.fromTriplets(n, n, triplets);

  return { JtJ, negJtr, cost, residuals };
}
```

**Verification**:
- Unit test with hand-crafted simple provider (e.g., f(x) = x - 3)
- Verify J^T J and J^T r match expected values
- No integration with solver yet

---

### Phase 3: Implement ONE Simple Provider (Fixed Point)

**Goal**: Prove the provider pattern works end-to-end.

Create `src/optimization/analytical/providers/fixed-point-provider.ts`:

```typescript
import { AnalyticalResidualProvider } from '../types';
import { fixed_point_x_grad, fixed_point_y_grad, fixed_point_z_grad } from '../../residuals/gradients/fixed-point-x-gradient';
// ... other imports

export function createFixedPointXProvider(
  pointXIndex: number,
  targetX: number
): AnalyticalResidualProvider {
  // Single variable dependency
  const variableIndices = pointXIndex >= 0 ? [pointXIndex] : [];

  return {
    variableIndices,

    computeResidual(variables: Float64Array): number {
      if (pointXIndex < 0) return 0;  // Locked, no residual
      return variables[pointXIndex] - targetX;
    },

    computeGradient(variables: Float64Array): Float64Array {
      // d/dx (x - target) = 1
      return new Float64Array([1]);
    },
  };
}

// Similar for Y and Z...
```

**Verification**:
- Unit test: compare analytical gradient to numerical gradient (finite differences)
- Unit test: accumulate single provider, verify J^T J is 1×1 matrix with value 1
- Unit test: verify residual matches plain computation

---

### Phase 4: Add Validation Path to transparentLM

**Goal**: Run analytical accumulation in parallel with autodiff. Compare results. Still use autodiff for actual solving.

Modify `src/optimization/autodiff-dense-lm.ts`:

```typescript
export interface TransparentLMOptions extends NonlinearLeastSquaresOptions {
  // ... existing options ...

  /**
   * When set, also compute normal equations analytically and validate
   * they match the autodiff computation. Does NOT use analytical for solving.
   */
  analyticalProviders?: AnalyticalResidualProvider[];
}

// In transparentLM(), after computing Jacobian via autodiff:
if (options.analyticalProviders) {
  const analyticalEqs = accumulateNormalEquations(
    variables.map(v => v.data),
    options.analyticalProviders
  );

  // Compare J^T J
  const autodiffJtJ = computeJtJFromDenseJacobian(jacobian);
  validateJtJMatches(autodiffJtJ, analyticalEqs.JtJ, tolerance);

  // Compare J^T r
  const autodiffJtr = computeJtr(jacobian, residuals);
  validateJtrMatches(autodiffJtr, analyticalEqs.negJtr, tolerance);
}
```

**Verification**:
- Pass fixed-point providers alongside autodiff
- Validation passes (or throws if mismatch)
- Actual optimization still uses autodiff (known-good)

---

### Phase 5: Add More Providers (One at a Time)

**Goal**: Incrementally add providers with parallel validation.

Order (simplest to most complex):
1. ✅ Fixed Point X/Y/Z (Phase 3)
2. Quaternion Norm
3. Distance
4. Line Length
5. Collinear X/Y/Z
6. Angle
7. Coplanar
8. Line Direction (9 variants)
9. Coincident Point X/Y/Z
10. Vanishing Line
11. Reprojection U/V (most complex, has distortion)

For each provider:
1. Create provider file
2. Unit test: gradient matches numerical
3. Integration test: add to validation path, verify matches autodiff
4. Commit

**Verification per provider**:
- Numerical gradient test passes
- Parallel validation in transparentLM passes
- Existing test suite still passes

---

### Phase 6: Wire ConstraintSystem to Create Providers

**Goal**: ConstraintSystem builds both residualFn (autodiff) and providers[] (analytical).

Modify `src/optimization/constraint-system/ConstraintSystem.ts`:

```typescript
// New method alongside existing solve()
buildAnalyticalProviders(layout: VariableLayout): AnalyticalResidualProvider[] {
  const providers: AnalyticalResidualProvider[] = [];

  // For each constraint type, create appropriate providers
  for (const point of this.fixedPoints) {
    providers.push(...createFixedPointProviders(point, layout));
  }
  // ... other constraint types ...

  return providers;
}
```

**Verification**:
- Parallel validation now uses real providers from ConstraintSystem
- All existing tests pass
- Validation passes on real optimization problems

---

### Phase 7: Use Analytical for Actual Solving ✓ COMPLETE

**Goal**: Switch from autodiff to analytical for the normal equations solve.

**Implementation** (completed):

1. Added `useAnalyticalSolve?: boolean` option to `TransparentLMOptions`
2. Created helper functions:
   - `solveFromNormalEquations()` - dense Cholesky from pre-computed J^T J
   - `solveSparseFromNormalEquations()` - sparse CG from pre-computed J^T J
   - `computeCostFromProviders()` - fast cost evaluation for step acceptance
3. Modified `transparentLM()` to:
   - When `useAnalyticalSolve=true`: use `accumulateNormalEquations()` instead of autodiff
   - Support both dense and sparse solve paths with analytical
   - Keep Value[] in sync for compatibility with existing code

**Verification**:
- ✅ 6 dedicated tests in `analytical-solve.test.ts`:
  - Distance constraints with analytical solve
  - Line direction constraints with analytical solve
  - Multiple constraints with analytical solve
  - Camera/reprojection with analytical solve
  - Sparse CG with analytical solve
  - Error handling (throws when providers missing)
- ✅ All 232 optimization tests pass (1 pre-existing failure)

---

### Phase 8: Make Analytical the Default

**Goal**: Flip the default. Autodiff becomes the fallback.

```typescript
// In solver config or ConstraintSystem
const defaultUseAnalytical = true;
```

**Verification**:
- Full test suite passes
- Real-world projects optimize correctly
- Performance is equal or better

---

### Phase 9: Remove Autodiff Code

**Goal**: Delete scalar-autograd dependency.

1. Remove `useAnalyticalSolve` option (always analytical)
2. Delete `computeJacobian()` function
3. Delete `solveDenseNormalEquations()` (if not needed)
4. Remove `scalar-autograd` from `package.json`
5. Delete any remaining `Value` imports

**Verification**:
- `npm run build` passes
- `npm test` passes
- `npm ls scalar-autograd` shows not installed

---

## Variable Layout

A key challenge is mapping entity variables to flat array indices:

```typescript
class VariableLayoutBuilder {
  private nextIndex = 0;
  private pointIndices = new Map<string, [number, number, number]>();
  private cameraPosIndices = new Map<string, [number, number, number]>();
  private cameraQuatIndices = new Map<string, [number, number, number, number]>();
  private values: number[] = [];

  addWorldPoint(point: WorldPoint): void {
    const effective = point.getEffectiveXyz();
    const indices: [number, number, number] = [
      point.lockedXyz.x !== undefined ? -1 : this.allocate(effective.x),
      point.lockedXyz.y !== undefined ? -1 : this.allocate(effective.y),
      point.lockedXyz.z !== undefined ? -1 : this.allocate(effective.z),
    ];
    this.pointIndices.set(point.id, indices);
  }

  private allocate(initialValue: number): number {
    this.values.push(initialValue);
    return this.nextIndex++;
  }

  build(): VariableLayout { /* ... */ }
}
```

This layout is built once at solve() start, then passed to all providers.

## Handling Locked Variables

When a variable is locked (e.g., point X coordinate):
1. Don't include it in the variable array (index = -1)
2. Provider skips that index in gradient accumulation
3. Residual computation uses the locked value directly (from entity, not variables array)

## Performance Benefits

| Operation | Autodiff | Analytical |
|-----------|----------|------------|
| Residual computation | Builds tape | Plain math |
| Gradient computation | Backward pass (all vars) | Direct call (~10 values) |
| Memory per residual | O(tape size) | O(1) |
| Jacobian storage | Dense M×N array | Never materialized |
| J^T J computation | O(M×N²) | O(M×k²) where k≈10 |

## Risks and Mitigations

### Risk: Gradient Bugs
**Mitigation**: gradient-script generates verified gradients. Parallel validation (Phases 4-7) catches any mismatch.

### Risk: Missing Residual Types
**Mitigation**: Audit all residual types in current system before starting. Generate any missing gradients.

### Risk: Variable Layout Complexity
**Mitigation**: The old explicit-jacobian system had `VariableLayout` - we can learn from that code (now deleted but in git history).

### Risk: Edge Cases (behind camera, division by zero)
**Mitigation**: Analytical gradients need same safeguards as autodiff. Test with same fixtures.

## Files to Create/Modify

**New files:**
- `src/optimization/analytical/types.ts` - Interfaces
- `src/optimization/analytical/accumulate-normal-equations.ts` - Core accumulation
- `src/optimization/analytical/providers/*.ts` - One file per residual type
- `src/optimization/analytical/variable-layout.ts` - Layout builder

**Modified files:**
- `src/optimization/autodiff-dense-lm.ts` - Add validation path (Phases 4-7)
- `src/optimization/constraint-system/ConstraintSystem.ts` - Build providers
- `src/optimization/fine-tune.ts` - Use analytical solver

**Deleted files (Phase 9):**
- Parts of `src/optimization/autodiff-dense-lm.ts`
- Remove `scalar-autograd` from dependencies

## Conclusion

The migration is incremental and verifiable:

1. **Phases 1-3**: Build infrastructure, test in isolation
2. **Phases 4-6**: Parallel validation catches bugs early
3. **Phases 7-8**: Gradual switch with fallback
4. **Phase 9**: Clean removal once confident

Each phase maintains backward compatibility and has clear verification criteria. The gradient functions already exist and are verified. This is a surgical replacement of the gradient computation layer, not a rewrite of the optimization system.
