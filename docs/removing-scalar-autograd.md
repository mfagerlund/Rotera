# Removing scalar-autograd: Migration to Analytical Gradients

## Overview

This document outlines the steps to remove the `scalar-autograd` dependency and replace automatic differentiation with analytical gradients generated by gradient-script. This migration will improve performance (no tape recording overhead) while maintaining the proven solver infrastructure.

## Current Architecture

### What scalar-autograd Does

1. **Value class**: Wraps numbers and tracks computation graph
2. **Backward pass**: Computes gradients via reverse-mode autodiff
3. **Jacobian computation**: In `autodiff-dense-lm.ts`, `computeJacobian()` calls `.backward()` on each residual to get gradients w.r.t. all variables

### Current Flow (in `transparentLM`)

```
Variables (Value[]) → Residual Function → Residuals (Value[])
                                              ↓
                              For each residual: .backward()
                                              ↓
                              Extract gradients from variables
                                              ↓
                              Build Jacobian matrix
                                              ↓
                              Solve normal equations (dense or sparse)
```

### What We Keep

- `transparentLM()` solver infrastructure
- Sparse matrix operations (`SparseMatrix`, `conjugateGradientDamped`)
- `ConstraintSystem` orchestration
- Entity-driven architecture (points/lines/constraints add themselves)

## Analytical Gradients Available

The following gradient functions are already generated in `src/optimization/residuals/gradients/`:

| Residual Type | Gradient Files |
|---------------|----------------|
| Reprojection U | `reprojection-u-gradient.ts`, `reprojection-u-dcam-gradient.ts` |
| Reprojection V | `reprojection-v-gradient.ts`, `reprojection-v-dcam-gradient.ts` |
| Distance | `distance-gradient.ts` |
| Angle | `angle-gradient.ts` |
| Collinear | `collinear-gradient.ts`, `collinear-x/y/z-gradient.ts` |
| Coplanar | `coplanar-gradient.ts` |
| Fixed Point | `fixed-point-gradient.ts`, `fixed-point-x/y/z-gradient.ts` |
| Line Direction | `line-direction-*-gradient.ts` (9 variants) |
| Line Length | `line-length-gradient.ts` |
| Coincident Point | `coincident-point-x/y/z-gradient.ts` |
| Quaternion Norm | `quat-norm-gradient.ts` |
| Vanishing Line | `vanishing-line-gradient.ts` |

## Migration Steps

### Phase 1: Create Analytical Jacobian Builder

Create `src/optimization/analytical-jacobian.ts`:

```typescript
interface AnalyticalResidualProvider {
  // Variable indices this residual depends on
  variableIndices: number[];

  // Compute residual value (plain number, not Value)
  computeResidual(variables: number[]): number;

  // Compute gradient w.r.t. each variable index
  // Returns array of length variableIndices.length
  computeGradient(variables: number[]): number[];
}
```

### Phase 2: Implement Residual Providers

For each residual type, create a provider that:
1. Takes variable indices (which slots in the variable array correspond to this residual's inputs)
2. Computes the residual value using plain math
3. Computes the gradient using the generated gradient functions

Example for reprojection:

```typescript
function createReprojectionUProvider(
  worldPointIndices: [number, number, number],
  cameraPosIndices: [number, number, number],
  quaternionIndices: [number, number, number, number],
  config: { fx, fy, cx, cy, k1, k2, k3, p1, p2, observedU, isZReflected }
): AnalyticalResidualProvider {
  return {
    variableIndices: [
      ...worldPointIndices,
      ...cameraPosIndices,
      ...quaternionIndices
    ],

    computeResidual(variables: number[]): number {
      // Plain math projection, return (predicted_u - observed_u)
    },

    computeGradient(variables: number[]): number[] {
      // Call reprojection_u_dcam_grad() from generated gradient
      // Map partial derivatives to correct variable indices
    }
  };
}
```

### Phase 3: Replace computeJacobian in transparentLM

Current (`autodiff-dense-lm.ts`):
```typescript
function computeJacobian(variables: Value[], residualFn): { jacobian, residuals } {
  const residualValues = residualFn(variables);
  for (let i = 0; i < numResiduals; i++) {
    residualValues[i].backward();
    for (let j = 0; j < numVariables; j++) {
      jacobian[i][j] = variables[j].grad;
    }
  }
}
```

New:
```typescript
function computeJacobianAnalytical(
  variables: number[],
  providers: AnalyticalResidualProvider[]
): { jacobian: number[][], residuals: number[] } {
  const jacobian: number[][] = [];
  const residuals: number[] = [];

  for (const provider of providers) {
    residuals.push(provider.computeResidual(variables));

    // Sparse row - only non-zero at variableIndices
    const row = new Array(variables.length).fill(0);
    const gradients = provider.computeGradient(variables);
    for (let i = 0; i < provider.variableIndices.length; i++) {
      row[provider.variableIndices[i]] = gradients[i];
    }
    jacobian.push(row);
  }

  return { jacobian, residuals };
}
```

### Phase 4: Update ConstraintSystem

Instead of building a `residualFn` that returns `Value[]`, build a list of `AnalyticalResidualProvider[]`:

```typescript
// Current
const residualFn = (vars: Value[]) => {
  const residuals: Value[] = [];
  for (const line of this.lines) {
    residuals.push(...line.computeResiduals(valueMap));
  }
  // ...
  return residuals;
};

// New
const providers: AnalyticalResidualProvider[] = [];
for (const line of this.lines) {
  providers.push(...line.createResidualProviders(variableLayout));
}
// ...
```

### Phase 5: Update Entity computeResiduals Methods

Each entity that contributes residuals needs a new method:

```typescript
// Current (in Line.ts)
computeResiduals(valueMap: ValueMap): Value[] {
  // Returns Value[] using scalar-autograd
}

// New
createResidualProviders(layout: VariableLayout): AnalyticalResidualProvider[] {
  // Returns providers that know their variable indices
  // and can compute residuals + gradients without autodiff
}
```

### Phase 6: Create New Solver Entry Point

Create `src/optimization/analytical-lm.ts`:

```typescript
export function analyticalLM(
  initialVariables: number[],
  providers: AnalyticalResidualProvider[],
  options: LMOptions
): LMResult {
  let variables = [...initialVariables];

  for (let iter = 0; iter < options.maxIterations; iter++) {
    const { jacobian, residuals } = computeJacobianAnalytical(variables, providers);

    // Same LM logic as transparentLM:
    // - Compute J^T J and J^T r
    // - Solve normal equations (dense or sparse)
    // - Update variables
    // - Check convergence
  }
}
```

### Phase 7: Parallel Validation Period

Run both solvers and compare:

```typescript
// In ConstraintSystem.solve()
const autodiffResult = this.solveWithAutodiff();
const analyticalResult = this.solveWithAnalytical();

// Compare solutions, log any divergence
validateResultsMatch(autodiffResult, analyticalResult);

return autodiffResult; // Use known-good result during validation
```

### Phase 8: Switch Default and Remove scalar-autograd

Once validated:
1. Change default to analytical solver
2. Remove autodiff code paths
3. Remove `scalar-autograd` from `package.json`
4. Delete `autodiff-dense-lm.ts` and related files

## Variable Layout

A key challenge is mapping entity variables to flat array indices:

```typescript
interface VariableLayout {
  // Returns indices for a world point's [x, y, z] in the variable array
  // Some may be -1 if locked
  getWorldPointIndices(point: WorldPoint): [number, number, number];

  // Returns indices for camera position [x, y, z]
  getCameraPosIndices(camera: IOptimizableCamera): [number, number, number];

  // Returns indices for quaternion [w, x, y, z]
  getCameraQuatIndices(camera: IOptimizableCamera): [number, number, number, number];
}
```

This layout is built once at solve() start, then passed to all providers.

## Handling Locked Variables

When a variable is locked (e.g., point X coordinate):
1. Don't include it in the variable array
2. Provider's gradient for that input is ignored (or index is -1)
3. Residual computation uses the locked value directly

## Performance Benefits

| Operation | Autodiff | Analytical |
|-----------|----------|------------|
| Residual computation | Builds tape | Plain math |
| Gradient computation | Backward pass | Direct function call |
| Memory | O(tape size) | O(1) per residual |
| Jacobian sparsity | Dense (computed) | Can exploit known sparsity |

## Risks and Mitigations

### Risk: Gradient Bugs
**Mitigation**: gradient-script generates verified gradients. Run numerical gradient checks during validation phase.

### Risk: Missing Residual Types
**Mitigation**: Audit all residual types in current system before starting. Generate any missing gradients.

### Risk: Variable Layout Complexity
**Mitigation**: The old explicit-jacobian system had `VariableLayout` - we can learn from that code (now deleted but in git history).

### Risk: Edge Cases (behind camera, division by zero)
**Mitigation**: Analytical gradients need same safeguards as autodiff. Test with same fixtures.

## Timeline Estimate

| Phase | Effort |
|-------|--------|
| Phase 1-2: Provider infrastructure | Medium |
| Phase 3-4: Solver integration | Medium |
| Phase 5: Entity updates | High (many entities) |
| Phase 6: New solver | Low (reuse LM logic) |
| Phase 7: Validation | Medium |
| Phase 8: Cleanup | Low |

## Files to Create/Modify

**New files:**
- `src/optimization/analytical-jacobian.ts` - Provider interface and Jacobian builder
- `src/optimization/analytical-lm.ts` - LM solver using analytical gradients
- `src/optimization/variable-layout.ts` - Maps entities to variable indices

**Modified files:**
- `src/entities/*/` - Add `createResidualProviders()` methods
- `src/optimization/constraint-system/ConstraintSystem.ts` - Use analytical solver
- `src/optimization/fine-tune.ts` - Use analytical solver

**Deleted files (after migration):**
- `src/optimization/autodiff-dense-lm.ts`
- Remove `scalar-autograd` from dependencies

## Conclusion

The migration is straightforward in concept: replace `.backward()` calls with direct gradient function calls. The main work is:

1. Building the provider infrastructure
2. Creating providers for each residual type
3. Handling the variable layout (which variables are free vs locked)

The gradient functions already exist and are verified. The LM solver logic stays the same. This is a surgical replacement of the gradient computation layer, not a rewrite of the optimization system.
